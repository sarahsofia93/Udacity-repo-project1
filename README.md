# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains Bank marketing data that includes information about clients of a bank (including age, job type, marital status etc. ). The goal is to predict whether a client will subscribe to a term deposit at the bank ("yes" or "no" classification).

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was a VotingEnsemble (accuracy = 0.91742) during the AutoML run.

For the scikit-learn run (logistic regression), the best performing model had an accuracy of 0.9088012 with max iterations = 100 and a regularization strength of 10.136.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline architecure is defined in the train.py file. First, the public dataset on bank marketing is transformed into a TabularDataset using TabularDatasetFactory. Using the clean_data function, the data is cleaned: All input parameters are converted to integer classes.  Binary classes (such as married) are encoded using 0 and 1. Inpute parameters with more than one class (e.g., jobs) are one hot encoded. Month and day of the week are mapped to numbers (1-12 and 1-7). The function returns the input data and the target variable separately.

For the hyperparameter tuning, the parameters "Inverse of the regularization strength" and "Maximum number of iterations to converge" are used. For the first, numbers in between 0.01 and 100 are chosen from a uniform distirbution. For the latter, a (random) choice between 50, 100 and 200 is made.

Finally, the data is split into train and test sets and then a logistic Regression is used on it, using accuracy as metric.


**What are the benefits of the parameter sampler you chose?**

Random sampling is usually a popular method because it allows an unbiased sampling of hyperparameters. It can be computationally more expensive than other samplers, but in the case of a light-weight classification task such as this, the random sample allows to go through many arbitrary combinaitons of parameters.

**What are the benefits of the early stopping policy you chose?**

The benefit of the Bandit Policy is that it terminates any runs where the primary metric is not within a certain range (slack range/slack amount) of the best performing training run. It is therefore helpful in saving computational cost for non-promising parameter combinations. (source: https://azure.github.io/azureml-sdk-for-r/reference/bandit_policy.html)

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best performing model by AutoML was a Voting Ensemble which combines several XGBoostClassifier and LightGBM algorithms.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The automl is better in accuracy, but only slighter. In contrast, the AutoML is computationally much more expensive. While the scikit-learn pipeline takes less than half an hour, we had to include an early stopping mechanism for the autoML pipeline in order to avoid it running forever. On the other hand, the autoML pipeline is able to run and compare several algorithms, includign ensemble learners, whereas we were limiting ourselves to one algorithm (logistic regression) with the scikit-learn. It seems for this simple classification task, the simple algorithm was enough to achieve an acceptable result.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

The sampling method could be changed. For example, I noticed for the sci-kit learn pipeline, all runs with the parameter max_iter = 50 were at the bottom of the resulting accuracy range. It therefore didn't make sense to try out so many combinations with that parameter setting. A more greedy sampler could've ignored this parameter value early on.

Other stopping policies could also be tried, or different parameters for the stopping policy. Due to lack of experience, the parameters and the policy was chosen arbitrarily. Choosing them more carefully could helpt focusing on good runs better.





## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

![plot](./cluster-deletion.png)

